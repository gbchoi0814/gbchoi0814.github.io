---
layout: post
title: (리뷰) Darts
---

### About this document
- 본 포스팅은 작성완료된 포스트다. 

- 본 포스팅은 **Liu, H., Simonyan, K., & Yang, Y. (2018). Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055** 을 리뷰한 것이다. 해당논문은 https://arxiv.org/abs/1806.09055 에서 확인할 수 있다. 

### Introduction 
- Liu(2018)은 딥러닝 아키텍쳐를 자동으로 골라주는 방법에 대하여 논의한 것이다. 예를들면 특정 데이터를 분석하는 아키텍처를 구성할때 노드와 노드사이를 $3\times 3$ convolution으로 연결할지 혹은 $5 \times 5$ max pooling으로 연결할지등을 알아서 선택해준다는 것이다. 

- 즉 이 논문은 "인공지능을 만드는 인공지능"에 대해서 논의한다고 볼 수 있다. 

- 개인적으로 "인간은 정말 끝없이 게을러지고 싶어하는구나, 이제 하다못해 인공지능을 만드는 것도 귀찮아서 그것까지 인공지능에게 시키는구나.." 하는 생각이 들게 만드는 논문이었다. 

- 처음에는 "인공지능을 만드는 인공지능"을 설계한다는 것에 상당히 거부감이 있었다. 거부감이 생긴 이유는 1) 사람이 데이터를 보고 그거에 맞게 구조를 설계해야 한다고 생각했으며 2) 자동으로 아키텍처를 선택하기 위해서 너무 많은 계산을 컴퓨터에게 시키지 않을까? 하는 생각이 들어서이다. (가뜩이나 딥러닝자체도 컴퓨터에게 부담을 주는데 말이다)

- 1)을 좀 더 자세히 설명하면 분석하고 싶은 자료의 특징을 잘 살펴보고 DNN을 쓸것인지 CNN을 쓸것인지 RNN을 쓸것인지 결정해야하며 학습속도를 높이기 위해서 전처리를 어떻게 하면 좋을지도 고민해야 한다는 것이다. 또한 자료의 형태와 분석 목적에 따라서 loss function을 어떻게 설정할지도 고민해야 한다. 그 밖에도 딥러닝 전문가가 설계해야할 많은 과정이 있다. 논문이 제안하는 방법은 이러한 경우에 대한 고민없이 이 설계부분을 그저 "아 몰라, 그냥 설계도 인공지능이 알아서 해줘"라는 식으로 보여서 처음에는 거부감이 들었다. 

- 하지만 논문을 읽어보니 내가 오해했다는 것을 알 수 있었다. 딥러닝에서는 위에서 말한것과 같은 대략적인 아키텍쳐를 전문가가 설계하더라도 세부적으로는 최적의 설계를 뽑아내기 위해 약간의 **"튜닝"** 작업이 필요하다. 예를들면 $3\times 3$ convolution 을 선택할지 $5 \times 5$ convolution 을 선택할지와 같은 식으로 말이다. 이러한 부분은 데이터에 대한 어떠한 직관도 필요하지 않다. 그저 설계자가 순수한 ''노가다''를 통해 경험적으로 터득할 수 있는 영역이다. Darts는 바로 이러한 영역에서 사용자의 노가다를 덜어주는 역할을 한다. 

- 논문에 대한 또다른 오해는 Darts가 컴퓨터를 너무 혹사시킨다는 것이었다. 그런데 어떻게 생각해보면 Darts가 아키텍처를 자동으로 선택하지 않으면 어차피 사람이 아키텍처를 이것저것 때려넣어보면서 튜닝을 해야하기 때문에 컴퓨터 입장에서 혹사 되는것은 비슷하다. 오히려 Darts가 사람보다 효율적으로 아키텍처를 찾아낼 확률이 높기 때문에 Darts가 컴퓨터를 덜 혹사시킨다고 생각할 수도 있다. 

- 다만 설계는 그대로 사용자의 손을 타야한다고 생각하는 입장에서 Darts가 그렇게 달갑지 않은 것은 사실이다. 따라서 이어질 글에서는 매우 비판적인 태도로 리뷰를 할 것이다. (나도 사람이기 때문에 어쩔 수 없다) 

### Darts (Differentialbe Architecture Search)

***Search Space***

- Darts에서 분석하고자 하는 자료는 **방향성비순환그래프(directed acyclic graph)**형태로 제한한다. 

- 방향성비순환그래프가 무엇인가? 방향성비순환그래프의 대표적인 예는 시계열 자료를 생각할 수 있다. 시계열 자료는 $X_t$시점에서 $X_{t+1}$의 시점으로 화살표가 있는(=방향성이 있는) 그래피컬 자료이다. 또한 방향성이 순환하지 않는 자료이다. 여기에서 $X_t$에서 $X_{t+1}$로의 방향성이 있다는 말은 $X_t$를 입력으로 하여 출력 $X_{t+1}$를 얻을 수 있다는 의미이며 순환하지 않는다는 말은 $X_t$에서 출발한 화살표를 따라서 어떠한 경로로 이동해도 $X_t$로 되돌아올 수 없다는 의미이다. 

- 방향성비순환그래프의 또 다른 예는 아래그림이다. 그림은 NN과 DNN 아키텍처를 설명하고 있다. (그림의 출처는 우클릭하여 얻을수 있다.)
<img src="https://d255esdrn735hr.cloudfront.net/graphics/9781787121423/graphics/B06923_09_41.png"/>

- 이 그림이 시사하는것은 임의의 딥러닝 아키텍처를 위와 같이 방향성비순환그래프의 형태로 표현가능하다는 것이다. 바로 이것이 저자들이 방향성비순환그래프로 자료의 형태를 제한한 이유이다. 앞으로는 그냥 편리하게 분석하고자 하는 자료의 형태를 위와같이 $N$개의 노드와 노드사이의 연결이 있는 DNN과 같은 아키텍처라고 생각하자. 

- 노테이션들을 정리해보자. 노드는 모두 $N$개가 있으며 각각의 노드는 
\begin{align}
x^{(1)}, x^{(2)}, \dots, x^{(N)}
\end{align}
와 같이 표시한다. 
그리고 $o^{(i,j)}(\cdot)$는 $j$번째 노드에서 $i$번째 노드로 가는 연산(operator)를 의미한다. 따라서 아래식이 성립한다. 
\begin{align}
x^{(i)}=\sum_{j<i} o^{(i,j)} \left( x^{(j)} \right)
\end{align}


***Continuous Relaxation and Optimization***


- 저자들이 원하는 것은 아래의 그림과 같이 아키텍쳐를 자동으로 골라주는 시스템을 구축하는 것이다. 
\begin{figure}
  \includegraphics[width=0.8\linewidth]{fig1.png}
  \caption*{\footnotesize \textcolor{red}{Liu(2018)에 수록된 그림, (a)에서 (c)로 갈수록 edge들 사이의 operation을 학습한다. 보는것처럼 그림에서는 총 3개(빨강, 파랑, 녹색)의 operation이 존재한다. (d)가 최종학습된 결과이다. }}
\end{figure}


### Reference
- Ahmed, K., & Torresani, L. (2017). Connectivity learning in multi-branch networks. arXiv preprint arXiv:1709.09582.
- Liu, C., Zoph, B., Neumann, M., Shlens, J., Hua, W., Li, L. J., ... & Murphy, K. (2018). Progressive neural architecture search. In Proceedings of the European Conference on Computer Vision (ECCV) (pp. 19-34).
- Liu, H., Simonyan, K., & Yang, Y. (2018). Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055.
- Real, E., Aggarwal, A., Huang, Y., & Le, Q. V. (2018). Regularized evolution for image classifier architecture search. arXiv preprint arXiv:1802.01548.
- Saxena, S., & Verbeek, J. (2016). Convolutional neural fabrics. In Advances in Neural Information Processing Systems (pp. 4053-4061).
- Shin, R., Packer, C., & Song, D. (2018). Differentiable Neural Network Architecture Search.
- Zoph, B., & Le, Q. V. (2016). Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578.
- Zoph, B., Vasudevan, V., Shlens, J., & Le, Q. V. (2018). Learning transferable architectures for scalable image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 8697-8710).
