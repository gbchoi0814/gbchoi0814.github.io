---
layout: post
title: (리뷰) Darts
---

### About this document
- 본 포스팅은 작성완료된 포스트다. 

- 본 포스팅은 **Liu, H., Simonyan, K., & Yang, Y. (2018). Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055** 을 리뷰한 것이다. 해당논문은 https://arxiv.org/abs/1806.09055 에서 확인할 수 있다. 

### Introduction 
- Liu(2018)은 딥러닝 아키텍쳐를 자동으로 골라주는 방법에 대하여 논의한 것이다. 예를들면 특정 데이터를 분석하는 아키텍처를 구성할때 노드와 노드사이를 $3\times 3$ convolution으로 연결할지 혹은 $5 \times 5$ max pooling으로 연결할지등을 알아서 선택해준다는 것이다. 

- 즉 이 논문은 "인공지능을 만드는 인공지능"에 대해서 논의한다고 볼 수 있다. 

- 개인적으로 "인간은 정말 끝없이 게을러지고 싶어하는구나, 이제 하다못해 인공지능을 만드는 것도 귀찮아서 그것까지 인공지능에게 시키는구나.." 하는 생각이 들게 만드는 논문이었다. 

- 처음에는 "인공지능을 만드는 인공지능"을 설계한다는 것에 상당히 거부감이 있었다. 거부감이 생긴 이유는 1) 사람이 데이터를 보고 그거에 맞게 구조를 설계해야 한다고 생각했으며 2) 자동으로 아키텍처를 선택하기 위해서 너무 많은 계산을 컴퓨터에게 시키지 않을까? 하는 생각이 들어서이다. (가뜩이나 딥러닝자체도 컴퓨터에게 부담을 주는데 말이다)

- 1)을 좀 더 자세히 설명하면 분석하고 싶은 자료의 특징을 잘 살펴보고 DNN을 쓸것인지 CNN을 쓸것인지 RNN을 쓸것인지 결정해야하며 학습속도를 높이기 위해서 전처리를 어떻게 하면 좋을지도 고민해야 한다는 것이다. 또한 자료의 형태와 분석 목적에 따라서 loss function을 어떻게 설정할지도 고민해야 한다. 그 밖에도 딥러닝 전문가가 설계해야할 많은 과정이 있다. 논문이 제안하는 방법은 이러한 경우에 대한 고민없이 이 설계부분을 그저 "아 몰라, 그냥 설계도 인공지능이 알아서 해줘"라는 식으로 보여서 처음에는 거부감이 들었다. 

- 하지만 논문을 읽어보니 내가 오해했다는 것을 알 수 있었다. 딥러닝에서는 위에서 말한것과 같은 대략적인 아키텍쳐를 전문가가 설계하더라도 세부적으로는 최적의 설계를 뽑아내기 위해 약간의 **"튜닝"** 작업이 필요하다. 예를들면 $3\times 3$ convolution 을 선택할지 $5 \times 5$ convolution 을 선택할지와 같은 식으로 말이다. 이러한 부분은 데이터에 대한 어떠한 직관도 필요하지 않다. 그저 설계자가 순수한 ''노가다''를 통해 경험적으로 터득할 수 있는 영역이다. Darts는 바로 이러한 영역에서 사용자의 노가다를 덜어주는 역할을 한다. 

- 논문에 대한 또다른 오해는 Darts가 컴퓨터를 너무 혹사시킨다는 것이었다. 그런데 어떻게 생각해보면 Darts가 아키텍처를 자동으로 선택하지 않으면 어차피 사람이 아키텍처를 이것저것 때려넣어보면서 튜닝을 해야하기 때문에 컴퓨터 입장에서 혹사 되는것은 비슷하다. 오히려 Darts가 사람보다 효율적으로 아키텍처를 찾아낼 확률이 높기 때문에 Darts가 컴퓨터를 덜 혹사시킨다고 생각할 수도 있다. 

- 다만 설계는 그대로 사용자의 손을 타야한다고 생각하는 입장에서 Darts가 그렇게 달갑지 않은 것은 사실이다. 따라서 이어질 글에서는 매우 비판적인 태도로 리뷰를 할 것이다. (나도 사람이기 때문에 어쩔 수 없다) 

### Darts (Differentialbe Architecture Search)

***Search Space***

- Darts에서 분석하고자 하는 자료는 **방향성비순환그래프(directed acyclic graph)**형태로 제한한다. 

- 방향성비순환그래프가 무엇인가? 방향성비순환그래프의 대표적인 예는 시계열 자료를 생각할 수 있다. 시계열 자료는 $X_t$시점에서 $X_{t+1}$의 시점으로 화살표가 있는(=방향성이 있는) 그래피컬 자료이다. 또한 방향성이 순환하지 않는 자료이다. 여기에서 $X_t$에서 $X_{t+1}$로의 방향성이 있다는 말은 $X_t$를 입력으로 하여 출력 $X_{t+1}$를 얻을 수 있다는 의미이며 순환하지 않는다는 말은 $X_t$에서 출발한 화살표를 따라서 어떠한 경로로 이동해도 $X_t$로 되돌아올 수 없다는 의미이다. 

- 방향성비순환그래프의 또 다른 예는 아래그림이다. 그림은 NN과 DNN 아키텍처를 설명하고 있다. (그림의 출처는 우클릭하여 얻을수 있다.)

<img src="https://d255esdrn735hr.cloudfront.net/graphics/9781787121423/graphics/B06923_09_41.png"/>

- 이 그림이 시사하는것은 임의의 딥러닝 아키텍처를 위와 같이 방향성비순환그래프의 형태로 표현가능하다는 것이다. 바로 이것이 저자들이 방향성비순환그래프로 자료의 형태를 제한한 이유이다. 앞으로는 그냥 편리하게 분석하고자 하는 자료의 형태를 위와같이 $N$개의 노드와 노드사이의 연결이 있는 DNN과 같은 아키텍처라고 생각하자. 

- 논문의 내용을 설명하기 위해서 노테이션들을 정리해보자. 노드는 모두 $N$개가 있으며 각각의 노드는 
\begin{align}
x^{(1)}, x^{(2)}, \dots, x^{(N)}
\end{align}
와 같이 표시한다. 
그리고 $o^{(i,j)}(\cdot)$는 $j$번째 노드에서 $i$번째 노드로 가는 연산(operator)를 의미한다. 따라서 아래식이 성립한다. 
\begin{align}
x^{(i)}=\sum_{j<i} o^{(i,j)} \left( x^{(j)} \right)
\end{align}

- 다음 섹션부터는 이러한 DNN 비슷한 아키텍쳐(=방향성비순환그래프)에서 어떻게 "인공지능을 설계하는 인공지능"을 만들 수 있는지 논의한다. 


***Continuous Relaxation and Optimization***


- 저자들이 원하는 것은 아래의 그림과 같이 아키텍쳐를 자동으로 골라주는 시스템을 구축하는 것이다. 
<img src="https://www.fast.ai/images/darts.png"/>

- 위의 그림은 Liu(2018)에 수록된 그림이다. (a)에서 (c)로 갈수록 노드들(=edge들) 사이의 operation을 학습한다. 여기에서 operation은 컨벌루션, max pooling등의 연산을 포함한다. 보는것처럼 그림에서는 총 3개(빨강, 파랑, 녹색)의 operation이 존재한다. (d)가 최종학습된 결과이다. 

- 위의 그림을 다시 설명하여 보자. ${\cal O}$를 어떠한 operation들의 집합이라고 하자. Liu(2018)의 Figure 1에서는 3개의 operation이 존재하므로 이경우 $\|{\cal O}\| =3$이다. 앞에서 말했듯이 $x^{(j)}$에서 $x^{(i)}$를 얻기 위해서는 operation $o^{(i,j)}$이 필요하다. 그런데 Liu(2018)의 Figure 1에서 보듯이 $o^{(i,j)}$는 하나의 연산만 존재하는 것이 아니라 총 $\|{\cal O}\|=3$개의 연산이 존재한다. 저자들의 아이디어는 이 연산들을 적당히 조합한 새로운 연산 $\bar{o}^{(i,j)}(\cdot)$를 찾아보자는 것이다. 

- 이때 새로운 연산 $\bar{o}^{(i,j)}(\cdot)$는 아래와 같이 기존연산들의 가중합으로 정의할 수 있다. 
\begin{align}
\bar{o}^{(i,j)}(x)=\sum_{o \in {\cal O}}\frac{\exp \left(\alpha_o^{(i,j)}\right)}{\sum_{o' \in {\cal O}} \exp\left( \alpha_{o'}^{(i,j)}\right)}o(x)
\end{align}
각 연산들의 가중치는 $\alpha_o^{(i,j)}$에 저장한다. 따라서 $\alpha_o^{(i,j)}$는 크기가 $\|{\cal O}\|=3$인 벡터가 된다. 고정된 $(i,j)$에 대하여 벡터 $\alpha_o^{(i,j)}$들이 span하는 공간을 search space라고 하는데 이 공간을 연속적으로 만들기 위하여 위와 같은 softmax함수를 고려한다. 

- 개인적으로 논문전체에서 가장 인상깊었던 부분이 바로 이 부분이었다. 소프트맥스 함수를 도입하여 벡터 
$\alpha_o^{(i,j)}$가 span하는 공간을 연속적이게 만든 부분이 논문중에서 가장 훌륭한 아이디어라 생각한다. 저자들은 이를 **continuous relaxation**이라고 표현하였다. 참고로 $\alpha_o^{(i,j)}$가 span하는 공간을 연속이라 생각하였기 때문에 미분을 정의할 수 있게 되었고 그 결과 추후에 소개할 Liu(2018)의 Algorithm 1이 동작할 수 있게 만들었다. (이는 일반적인 logistic regression을 수행하는 아이디어와 동일함) 또한 이 부분으로 인하여 아키텍쳐를 설계하는 일을 단순한 multiclass classification 문제로 치환할 수 있었다. 따라서 바로 이런 부분이 "AI를 만들어내는 AI" 라는 슬로건이 가능한 이유이라 생각한다. (물론 Sexena(2016), Ahmed(2017), Shin(2018)등이 비슷한 아이디어를 냈었지만 저자들의 방법은 CNN, RNN계열에 모두 사용할수 있고 나아가 Graph Topology로 표현가능한 어떠한 형태의 아티텍쳐로도 확장가능하다는 점에서 차별성을 가진다.)

- 우리의 목적은 $\alpha=\left\\{ \alpha^{(i,j)} \right\\}$를 학습하는 것이다. (그리고 이때의 $\alpha$를 (encoding of the) architecture라고 함)이는 결국 최적의 성능을 내는 operation들의 조합을 찾는것과 동치이다. 추후에 $\bar{o}^{(i,j)}$를 아래와 같이 
\begin{align}
o^{(i,j)}=\underset{o \in {\cal O}}{\operatorname{argmax}} \alpha_o^{(i,j)}
\end{align}
로 대치한다. 이것은 Liu(2018)의 Figure 1 (c)에서 Figure 1 (d)로 이동하는 과정을 의미한다. 

- "AI를 만들어내는 AI"를 구현하기 위해서는 최적의 성능을 내는 operation들의 조합 $\alpha=\left\\{ \alpha^{(i,j)}\right\\}$와 그에 대응하는 웨이트 $w$를 동시에 학습하는 손실함수를 고려해야 한다. 따라서 아래와 같은 최적화 문제를 고려한다. 
\begin{align}
\min_ {\alpha} {\cal L}_ {val}(w^* (\alpha),\alpha) ~~~ s.t ~~~ w^* (\alpha)=\underset{w}{\operatorname{argmin}} {\cal L}_ {train}(w,\alpha)
\end{align}

- 위의 식에서 ${\cal L}_ {val}$은 **validation loss**를, ${\cal L}_ {train}$은 **training loss**를 의미한다. 위의 식은 제약조건이 있는 최적화 문제이며 비선형방정식이므로 이 방정식을 푸는것은 어렵다(=못푼다). 따라서 이러한 문제를 해결하기 위해서는 알고리즘적으로 접근하여 해를 근사시켜야 한다. 구체적인 알고리즘은 다음섹션에서 설명한다. 


***Approximation***


- 저자들은 이를 풀기위해서 $(w,\alpha)$를 iterative하게 update하는 방법을 제안하였다. 즉 저자들은 아래의 알고리즘을 제안하였다. 

- 


### Reference
- Ahmed, K., & Torresani, L. (2017). Connectivity learning in multi-branch networks. arXiv preprint arXiv:1709.09582.
- Liu, C., Zoph, B., Neumann, M., Shlens, J., Hua, W., Li, L. J., ... & Murphy, K. (2018). Progressive neural architecture search. In Proceedings of the European Conference on Computer Vision (ECCV) (pp. 19-34).
- Liu, H., Simonyan, K., & Yang, Y. (2018). Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055.
- Real, E., Aggarwal, A., Huang, Y., & Le, Q. V. (2018). Regularized evolution for image classifier architecture search. arXiv preprint arXiv:1802.01548.
- Saxena, S., & Verbeek, J. (2016). Convolutional neural fabrics. In Advances in Neural Information Processing Systems (pp. 4053-4061).
- Shin, R., Packer, C., & Song, D. (2018). Differentiable Neural Network Architecture Search.
- Zoph, B., & Le, Q. V. (2016). Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578.
- Zoph, B., Vasudevan, V., Shlens, J., & Le, Q. V. (2018). Learning transferable architectures for scalable image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 8697-8710).
