---
layout: post
title: 매우 얕은 통계학
---

### 통계학의 과제
통계나 머신러닝을 통하여 해결하고자 하는 문제는 아래와 같은 것들이 있다. 

***Regression***
- $y=f(x)$형태의 모델임. 
- $\\{(x_i,y_i)\\}_ {i=1}^{n}$에서 $f$를 학습하는 방식임. 
- 회귀는 결국 function estimation. 

***Classification***
- $y=f(x)$형태의 자료에서 $y=0$ 또는 $1$인 형태이므로 회귀모델로 볼 수 있음. 

***Anomaly detection***
- 주어진 입력표본 $\\{x_i\\}_ {i=1}^{n}$에 포함된 비정상적인 값을 발견하는 문제이다. 
- 밀집해있는 자료를 정상으로 간주하고 그 군집으로부터 멀리 떨어진 자료를 비정상으로 간주한다. (Line형태로 군집이 이루어지면 어쩔꺼야? ㅋㅋ) 

***Clustering*** 
- 표본간의 유사도를 측정하는 방법을 적절히 선택하는 것이 중요하다. 
- 이때 그래프라플라시안을 활용하면 좋을듯? smoothness개념 
- hst에서도 전체 smoothness를 정의하는 어떠한 방법이 있으면 좋겠네. Total variation의 개념과도 연결되는것 같다? 

***Dimensionality reduction***
- 입력차원 $\\{x_i\\}_ {i=1}^{n}$의 차원이 매우 클때 그것을 낮은차원의 자료들 $\\{z_i\\}_ {i=1}^{n}$로 바꾸는 방법
- 차원축소를 수행하는 연산이 선형이면 적당한 행렬 ${\bf T}$를 사용하여 $z_i=x_i{\bf T}$와 같은 방식으로 쓸 수 있음. 
- 가령 예를 들어서 $x_i=[1,2,4,5,8]$과 같은 자료가 있다고 하자. 이때 $x_i$의 차원은 $1\times 5$이다. 그러면 ${\bf T}$를 $5\times 2$와 같은 행렬을 잡아와서 $z_i=x_i{\bf T}$와 같이 쓸 수 있는데 이러면 $z_i$의 차원은 $1 \times 2$가 되고 ${\bf T}$는 5차원자료를 2차원으로 줄이는 변환이 된다. 

### 통계학의 계파 
위에 제시된 문제를 해결하는데에는 다양한 철학을 가진 학파가 있다. 

***Generative model vs Discriminative model***
- $28\times 28$-pixel 의 이미지자료가 있다고 하자. 따라서 이 경우 ${\bf X}_ {n\times 784}$가 된다. 
- $y_{n\times 1}$이라고 하고, $y \in \\{0,\dots,9\\}$라고 하자. 
- 어쨋든 ${\bf X}$와 $y$를 통하여 conditional pdf $f(y|{\bf X})$를 알 수 있다. 결국 분류문제는 아래식을 풀어서 $\hat{y}$을 구하는 과정으로 이해할 수 있다. 
  \begin{align}
  \hat{y}=\underset{y}{\operatorname{argmax}} f(y|{\bf X})
  \end{align}
- 그런데 위에서 $f(y|{\bf X})$를 최대화하는 대신에 $f({\bf X},y)$를 최대화해도 문제없는데 그 이유는 아래식이 성립하기 때문이다. 
  \begin{align}
  f(y|{\bf X})=\frac{f({\bf X},y)}{f({\bf X})} \propto f({\bf X},y)
  \end{align}
  이처럼 $\hat{y}$는 $f({\bf X},y)$를 최대화 하거나 $f(y|{\bf X})$를 최대화 함으로써 구할 수 있는데 $f({\bf X},y)$를 최대화 하여 구하자는 주의가 *Generative model*을 지지하는 사람들이고  $f(y|{\bf X})$를 최대화 하여 구하자는 사람들은 *Discriminative model*을 지지하는 사람들이다.  
- 일반적으로 $f({\bf X},y)$를 알면 $f(y|{\bf X})$를 쉽게 계산할 수 있지만(아래식참고) 반대는 불가능하므로 $f({\bf X},y)$를 아는게 더 어려운 일이다. 
  \begin{align}
  f(y|{\bf X})=\frac{f({\bf X},y)}{f({\bf X})}=\frac{f({\bf X},y)}{\sum_y f({\bf X},y)}
  \end{align}
- 따라서 더 구하기 힘든함수 $f({\bf X},y)$를 알아내서 $\hat{y}$를 구하는 것보다는 좀 더 구하기 쉬운 함수 $f(y \vert {\bf X})$ 를 알아낸 다음에 $\hat{y}$을 구하는 것이 더 효과적이다. 이것이 SVM을 창시한 뱁닉(Vapnik)의 아이디어이다. 


***Frequentis vs Bayesian***
- Frequentist는 $\theta$를 확률변수로 보지 않는다. (fixed paramater라고 생각함.)
- Bayesian은 $\theta$를 확률변수라고 생각한다. 
- 따라서 Frequentist는 $\theta$의 값을 MLE와 같은 방법으로 구하려고 하지만 Bayesian는 $\theta$의 (posterior) distribution 관심이 있다. 


## 모델링 하는 방법
- 가장기본적인 모델인 1차원 회귀식으로 부터 시작하자. 
\begin{align}
y_i=\beta_0+x_i\beta_1+\epsilon_i
\end{align}
위의 모델은 디자인매트릭스를 $y_i$를 설명하기 위한 basis를 $[1,x_i]$로 선택하였으며, 각 basis를 선형결합하여 (계수는 $\beta_0$, $\beta_1$) $y_i$를 표현하였다. 

- 위의 모델을 좀 더 개신시켜 basis를 $[1,\cos x_i, \sin x_i, \cos 2x_i, \sin 2x_i, \dots, ]$와 같이 확장해볼 수도 있을 것이다. 이때 각각의 basis는 오소고날하다. 

- 위의 2가지 방법은 각각 어떠한 가정을 내포한다. 가령 ''$y_i$는 $x_i$의 선형결합으로 만들어질 수 있다.'' 라든가 하는식으로 말이다. 즉 
\begin{align}
y_i=f(x_i)+\epsilon
\end{align}
와 같은 모델에서 $f$가 어떠한 형태를 가질것인지 미리 알고있다고 생각한다. 이처럼 $f$가 어떤 모양인지 미리 알고 접근하는 방법을 파라메트릭 모델이라고 한다. 그리고 보통 $f$의 모양을 결정하는 과정(즉 적절한 basis를 선택하는 과정)을 모델링이라고 한다. 

- $f$를 어떻게 모델링할지 감이 안올수도 있다. 즉 자료를 봤는지 선형의 모양을 가지는지 어떤지 감을 못잡겠는 경우이다. 이것을 바꾸어 말하면 $\\{y_i\\}$가 $\\{x_i\\}$의 어떤 space에 있는지 감을 못 잡겠다는 뜻이다. 혹은 모델링이 귀찮을 수도 있다. 이럴 경우 $\\{y_i\\}$가 $\\{x_i\\}$의 어떤 특정스페이스 $\cal A$의 부분공간에 존재한다고 가정하고 그 특정스페이스 $\cal A$를 expansion할 수 있는 베이시스를 선택하여 문제를 풀 수 있다. 가령 예를들면 ''$\\{y_i\\}$가 어떤 공간에 있는지 모르겠는데 최소한 비숍스페이스의 부분공간에 있는것 같아'' 라고 생각한다면 웨이블릿 베이시스를 선택하여 모델링 하는 것이다. 

- 커널모델을 사용하여 자료를 표현하는 것도 같은 맥락으로 이해할 수 있다. 
