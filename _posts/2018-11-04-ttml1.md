---
layout: post
title: 통계학, 매우 얕은 지식
category: ttml
---

### 머신러닝 과업들
통계나 머신러닝을 통하여 해결하고자 하는 문제는 아래와 같은 것들이 있다. 

***Regression***
- $y=f(x)$형태의 모델임. 
- $\\{(x_i,y_i)\\}_ {i=1}^{n}$에서 $f$를 학습하는 방식임. 
- 회귀는 결국 function estimation. 

***Classification***
- $y=f(x)$형태의 자료에서 $y=0$ 또는 $1$인 형태이므로 회귀모델로 볼 수 있음. 

***Anomaly detection***
- 주어진 입력표본 $\\{x_i\\}_ {i=1}^{n}$에 포함된 비정상적인 값을 발견하는 문제이다. 
- 밀집해있는 자료를 정상으로 간주하고 그 군집으로부터 멀리 떨어진 자료를 비정상으로 간주한다. (Line형태로 군집이 이루어지면 어쩔꺼야? ㅋㅋ) 

***Clustering*** 
- 표본간의 유사도를 측정하는 방법을 적절히 선택하는 것이 중요하다. 
- 이때 그래프라플라시안을 활용하면 좋을듯? smoothness개념 
- hst에서도 전체 smoothness를 정의하는 어떠한 방법이 있으면 좋겠네. Total variation의 개념과도 연결되는것 같다? 

***Dimensionality reduction***
- 입력차원 $\\{x_i\\}_ {i=1}^{n}$의 차원이 매우 클때 그것을 낮은차원의 자료들 $\\{z_i\\}_ {i=1}^{n}$로 바꾸는 방법
- 선형차원축소의 경우 가로로 긴 행렬 ${\bf T}$를 사용하여 $z_i={\bf T}x_i$와 같은 방식으로 쓸 수 있음. 행렬이 가로로 긴 이후는 $z_i$의 차원이 $x_i$보다 작아야 하기 때문임. 
- 가령 예를 들어서 $x_i=[1,2,4,5,8]$과 같은 자료가 있다고 하자. 이때 ${\bf x}$의 차원은 5이다. 그러면 ${\bf T}$를 $5\times 2$와 같은 행렬을 잡아와서 $z_i={\bf T}x_i$와 같이 쓸 수 있는데 이러면 $z_i$의 차원은 2가 되고 ${\bf T}$는 5차원자료를 2차원으로 줄이는 변환이 된다. 


### 학파 
위에 제시된 문제를 해결하는데에는 다양한 철학을 가진 학파가 있다. 

***Generative model vs Discriminative model***
- $28\times 28$-pixel 의 이미지자료가 있다고 하자. 따라서 이 경우 ${\bf X}_ {n\times 784}$가 된다. 
- $y_{n\times 1}$이라고 하고, $y \in \\{0,\dots,9\\}$라고 하자. 
- 어쨋든 ${\bf X}$와 $y$를 통하여 conditional pdf $f(y|{\bf X})$를 알 수 있다. 결국 분류문제는 아래식을 풀어서 $\hat{y}$을 구하는 과정으로 이해할 수 있다. 
  \begin{align}
  \hat{y}=\underset{y}{\operatorname{argmax}} f(y|{\bf X})
  \end{align}
- 그런데 위에서 $f(y|{\bf X})$를 최대화하는 대신에 $f({\bf X},y)$를 최대화해도 문제없는데 그 이유는 아래식이 성립하기 때문이다. 
  \begin{align}
  f(y|{\bf X})=\frac{f({\bf X},y)}{f({\bf X})} \propto f({\bf X},y)
  \end{align}
  이처럼 $\hat{y}$는 $f({\bf X},y)$를 최대화 하거나 $f(y|{\bf X})$를 최대화 함으로써 구할 수 있는데 $f({\bf X},y)$를 최대화 하여 구하자는 주의가 *Generative model*을 지지하는 사람들이고  $f(y|{\bf X})$를 최대화 하여 구하자는 사람들은 *Discriminative model*을 지지하는 사람들이다.  
- 일반적으로 $f({\bf X},y)$를 알면 $f(y|{\bf X})$를 쉽게 계산할 수 있지만(아래식참고) 반대는 불가능하므로 $f({\bf X},y)$를 아는게 더 어려운 일이다. 
  \begin{align}
  f(y|{\bf X})=\frac{f({\bf X},y)}{f({\bf X})}=\frac{f({\bf X},y)}{\sum_y f({\bf X},y)}
  \end{align}
- 따라서 더 구하기 힘든함수 $f({\bf X},y)$를 알아내서 $\hat{y}$를 구하는 것보다는 좀 더 구하기 쉬운 함수 $f(y \vert {\bf X})$ 를 알아낸 다음에 $\hat{y}$을 구하는 것이 더 효과적이다. 이것이 SVM을 창시한 뱁닉(Vapnik)의 아이디어이다. 


***Frequentis vs Bayesian***
- Frequentist는 $\theta$를 확률변수로 보지 않는다. (fixed paramater라고 생각함.)
- Bayesian은 $\theta$를 확률변수라고 생각한다. 
- 따라서 Frequentist는 $\theta$의 값을 MLE와 같은 방법으로 구하려고 하지만 Bayesian는 $\theta$의 (posterior) distribution 관심이 있다. 
