---
layout: post
title: 매우 얕은 통계학
---

### 통계학의 과제
통계나 머신러닝을 통하여 해결하고자 하는 문제는 아래와 같은 것들이 있다. 

***Regression***
- $y=f(x)$형태의 모델임. 
- $\\{(x_i,y_i)\\}_ {i=1}^{n}$에서 $f$를 학습하는 방식임. 
- 회귀는 결국 function estimation. 

***Classification***
- $y=f(x)$형태의 자료에서 $y=0$ 또는 $1$인 형태이므로 회귀모델로 볼 수 있음. 

***Anomaly detection***
- 주어진 입력표본 $\\{x_i\\}_ {i=1}^{n}$에 포함된 비정상적인 값을 발견하는 문제이다. 
- 밀집해있는 자료를 정상으로 간주하고 그 군집으로부터 멀리 떨어진 자료를 비정상으로 간주한다. (Line형태로 군집이 이루어지면 어쩔꺼야? ㅋㅋ) 

***Clustering*** 
- 표본간의 유사도를 측정하는 방법을 적절히 선택하는 것이 중요하다. 
- 이때 그래프라플라시안을 활용하면 좋을듯? smoothness개념 
- hst에서도 전체 smoothness를 정의하는 어떠한 방법이 있으면 좋겠네. Total variation의 개념과도 연결되는것 같다? 

***Dimensionality reduction***
- 입력차원 $\\{x_i\\}_ {i=1}^{n}$의 차원이 매우 클때 그것을 낮은차원의 자료들 $\\{z_i\\}_ {i=1}^{n}$로 바꾸는 방법
- 차원축소를 수행하는 연산이 선형이면 적당한 행렬 ${\bf T}$를 사용하여 $z_i=x_i{\bf T}$와 같은 방식으로 쓸 수 있음. 
- 가령 예를 들어서 $x_i=[1,2,4,5,8]$과 같은 자료가 있다고 하자. 이때 $x_i$의 차원은 $1\times 5$이다. 그러면 ${\bf T}$를 $5\times 2$와 같은 행렬을 잡아와서 $z_i=x_i{\bf T}$와 같이 쓸 수 있는데 이러면 $z_i$의 차원은 $1 \times 2$가 되고 ${\bf T}$는 5차원자료를 2차원으로 줄이는 변환이 된다. 

### 통계학의 계파 
위에 제시된 문제를 해결하는데에는 다양한 철학을 가진 학파가 있다. 

***Generative model vs Discriminative model***
- $28\times 28$-pixel 의 이미지자료가 있다고 하자. 따라서 이 경우 ${\bf X}_ {n\times 784}$가 된다. 
- $y_{n\times 1}$이라고 하고, $y \in \\{0,\dots,9\\}$라고 하자. 
- 어쨋든 ${\bf X}$와 $y$를 통하여 conditional pdf $f(y|{\bf X})$를 알 수 있다. 결국 분류문제는 아래식을 풀어서 $\hat{y}$을 구하는 과정으로 이해할 수 있다. 
  \begin{align}
  \hat{y}=\underset{y}{\operatorname{argmax}} f(y|{\bf X})
  \end{align}
- 그런데 위에서 $f(y|{\bf X})$를 최대화하는 대신에 $f({\bf X},y)$를 최대화해도 문제없는데 그 이유는 아래식이 성립하기 때문이다. 
  \begin{align}
  f(y|{\bf X})=\frac{f({\bf X},y)}{f({\bf X})} \propto f({\bf X},y)
  \end{align}
  이처럼 $\hat{y}$는 $f({\bf X},y)$를 최대화 하거나 $f(y|{\bf X})$를 최대화 함으로써 구할 수 있는데 $f({\bf X},y)$를 최대화 하여 구하자는 주의가 *Generative model*을 지지하는 사람들이고  $f(y|{\bf X})$를 최대화 하여 구하자는 사람들은 *Discriminative model*을 지지하는 사람들이다.  
- 일반적으로 $f({\bf X},y)$를 알면 $f(y|{\bf X})$를 쉽게 계산할 수 있지만(아래식참고) 반대는 불가능하므로 $f({\bf X},y)$를 아는게 더 어려운 일이다. 
  \begin{align}
  f(y|{\bf X})=\frac{f({\bf X},y)}{f({\bf X})}=\frac{f({\bf X},y)}{\sum_y f({\bf X},y)}
  \end{align}
- 따라서 더 구하기 힘든함수 $f({\bf X},y)$를 알아내서 $\hat{y}$를 구하는 것보다는 좀 더 구하기 쉬운 함수 $f(y \vert {\bf X})$ 를 알아낸 다음에 $\hat{y}$을 구하는 것이 더 효과적이다. 이것이 SVM을 창시한 뱁닉(Vapnik)의 아이디어이다. 


***Frequentis vs Bayesian***
- Frequentist는 $\theta$를 확률변수로 보지 않는다. (fixed paramater라고 생각함.)
- Bayesian은 $\theta$를 확률변수라고 생각한다. 
- 따라서 Frequentist는 $\theta$의 값을 MLE와 같은 방법으로 구하려고 하지만 Bayesian는 $\theta$의 (posterior) distribution 관심이 있다. 


## 모델링하는 방법
- $\\{(x_i,y_i)\\}_ {i=1}^{n}$을 관측하였다고 하자. 우리가 풀고 싶은 문제는
\begin{align}
y_i=f(x_i)+\epsilon_i
\end{align}
로 표현했을때 어색하지 않은 underlying function $f(x)$를 찾아내는 것이다. 이러한 $f(x)$를 찾으면 모델이 만들어 지고 결국 새로운 관측치 $x_{i^* }$에 대한 prediction을 할 수 있다. 

- 가장 기본적인 모델인 1차원 회귀식으로 부터 시작하자. 
\begin{align}
y_i=f(x_i)+\epsilon_i, ~~~ f(x)=\beta_0+x\beta_1
\end{align}
위의 모델은 디자인매트릭스를 $f(x)$를 설명하기 위한 basis를 $[1,x]$로 선택하였다. 

- 위의 모델을 좀 더 개신시켜 basis를 $[1,\cos x, \sin x, \cos 2x, \sin 2x, \dots, ]$와 같이 확장해볼 수도 있을 것이다. 이때 각각의 basis는 오소고날하다. 

- 위의 2가지 방법은 각각 어떠한 가정을 내포한다. 가령 ''$f(x)$는 $x$에 선형변환으로 만들어질 수 있다. (즉 $(f(x)=\beta_0+\beta_1x)$)'' 라든가 하는식으로 말이다. 이말은 
\begin{align}
y_i=f(x_i)+\epsilon, 
\end{align}
와 같은 모델에서 $f$가 어떠한 형태를 가질것인지 미리 알고 있다고 생각한다. 이처럼 $f$가 어떤 모양인지 미리 알고 접근하는 방법을 파라메트릭 모델이라고 한다. 그리고 보통 $f$의 모양을 결정하는 과정(즉 적절한 basis를 선택하는 과정)을 모델링이라고 한다. 

- $f$를 어떻게 모델링할지 감이 안 올 수도 있다. 즉 자료를 봤는지 선형의 모양을 가지는지 어떤지 감을 못잡겠는 경우이다. 이것을 바꾸어 말하면 $\\{y_i\\}$가 $\\{x_i\\}$의 어떤 space에 있는지 감을 못 잡겠다는 뜻이다. 혹은 모델링이 귀찮을 수도 있다. 이럴 경우 $f(x)$가 $x$의 어떤 특정스페이스 $\cal A$의 부분공간에 존재한다고 가정하고 그 특정스페이스 $\cal A$를 expansion할 수 있는 베이시스를 선택하여 문제를 풀 수 있다. 가령 예를들면 ''$f(x)$가 어떤 공간에 있는지 모르겠는데 최소한 비숍스페이스의 부분공간에 있는것 같아'' 라고 생각한다면 웨이블릿 베이시스를 선택하여 모델링 하는 것이다. 

- 보통위와 같은 접근법은 무한대의 basis를 활요한다. 수학적으로 증명된 내용은 대부분 "이런식으로 무한개의 basis를 활용하면 특정공간에 있는 어떠한 함수도 표현할수 있어요~"라는 식이다. 요렇게 $f$를 표현하는데 무한개의 basis를 활용하는 모델링을 semi-parametric이라고 한다. 

- 커널모델을 사용하여 자료를 표현하는 경우는 어떤가? 커널모델은 $f(x)$를 아래와 같이 가정하는 것으로 이해할 수 있다. 
\begin{align}
f(x)=\theta_1K_h(x,x_i)+\dots+\theta_n K_h(x,x_n)
\end{align}
특이한것은 $f(x)$를 설계할때 관측자료 $x_i$를 사용하였다는 점이다. 퓨리에 변환이 $f(x)$를 가정하는 방식
\begin{align}
f(x)=\theta_0+\theta_1 e^{i\omega x)+\theta_2 e^{i\omega 2x) + \dots
\end{align}
과 비교하여 보면 퓨리에변환은 $f(x)$를 표현하는데 무한대에 가까운 숫자의 basis를 사용하였지만 입력자료 $x_i$를 basis에 활용하지는 않았다. 이처럼 입력값 $x_i$를 직접 basis를 설계하는데 활용하는 방식을 non-parametric이라고 한다. 
