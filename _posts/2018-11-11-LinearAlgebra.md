---
layout: post 
title: (모르면 창피한) 선형대수학
---

선형대수학은 학부 1학년에 배우는 매우 쉬운 과목이지만 기본적인 개념이 헷갈릴때가 있다. 그럴때 마다 다시 선형대수학책을 꺼내들고 공부하게 되는데 이러한 무의미한 반복을 줄이고 싶어서 본 문서를 작성하게 되었다. 본 문서에서 정리하는 선형대수학은 통계학과 연관성이 있는 내용위주로 정리함을 미리 밝힌다. 

### Eigenvalues and Eigenvectors
- 임의의 정사각행렬 ${\bf A}_ {n \times n}$에 대하여 어떠한 벡터 ${\bf v}_ {n \times 1} \neq {\bf 0}$가 적당한값 $\lambda$에 대하여 아래식을 만족하면 $\bf v$를 $\bf A$의 고유벡터라고 한다. 
\begin{align}
{\bf A v}= \lambda {\bf v}
\end{align}
주의할것은 $0$-벡터는 고유벡터로 취급하지 않는다는 것이다. 

- **고유값이 없는 정사각행렬은 없다.**
고유값이 없다는 말은 $det({\bf A}-\lambda {\bf I})=0$을 만족하는 $\lambda$가 없다는 말인데 임의의 $n$차 다항식의 해는 항상 존재하므로(정확하게는 모르겠지만 왠지 이런 정리가 있을것 같다) 어떠한 정사각행렬 ${\bf A}_ {n \times n}$도 $n$개의 고유값을 가진다. 

- **고유벡터가 없는 정사각행렬은 없다.**
일단 ${\bf A}_ {n \times n}$는 항상 $n$개의 고유값을 가진다. 그중에서 하나의 고유값 $\lambda^* $를 fix했다고 하자. 그러면 고유벡터가 없다는 말은 
\begin{align}
\left({\bf A}-\lambda^* {\bf I}\right)=0
\end{align}
를 만족하는 벡터 $\bf v$는 오직 $\bf v=0$인 경우일 때 뿐이란 것을 의미한다. 그런데 고유값의 정의상 $det({\bf A}-\lambda^* {\bf I})=0$이 된다. 따라서 행렬 ${\bf A}-\lambda^* {\bf I}$은 *sing*-매트릭스가 된다. 따라서 ${\bf A}-\lambda^* {\bf I}$의 모든 row는 일차독립이 아니다. 따라서  $({\bf A}-\lambda^* {\bf I})\bf v=0$을 만족하는 $\bf v \neq 0$ 인 벡터가 적어도 하나는 존재한다. 이것은 고유벡터의 $\bf v$ 정의를 만족하므로 고유벡터가 없는 정사각행렬은 없다. 

- 따라서 모은 정사각행렬은 고유벡터와 고유값을 반드시 가진다. 

- ***tri*-매트릭스의 고유값은 대각선값들이다.** 이 사실은 그냥 일단 외우자. 이 사실을 이용하면 $\bf A=I$일 경우와 $\bf A=O$일 경우의 고유값을 쉽게 구할 수 있다. $\bf A=I$의 고유값은 $1$이고 고유벡터는 ***all non-zero vectors***다. 그리고 $\bf A=O$의 고유값은 $0$이고 고유벡터는 ***all non-zero vectors***다. 이러한 사실들이 말해주는 것은 고유벡터가 무한개일 수도 있다는 것이다. 

- **모든 고유값들의 합은 원래 행렬의 trace와 같고 모든 고유값들의 곱은 원래 행렬의 determinent와 같다. 이 사실은 임의의 정사각행렬에서 성립한다.** 즉 임의의 정사각행렬에서 $tr({\bf A}_ {n \times n})=\sum_{i=1}^{n} \lambda_i$이고 $det({\bf A}_ {n \times n})=\prod_{i=1}^{n}\lambda_i$이다. 이 사실도 그냥 증명없이 외우자. 

- ***sing*-매트릭스의 고유값에는 0이 적어도 하나는 포함되어 있다.** 이 사실은 **모든 고유값들의 곱은 원래 행렬의 determinent와 같다**는 사실을 떠올리면 쉽게 이해할 수 있다. 

- $A_{n \times n}$모든 col의 합이 1인 행렬을 *markov*-매트릭스라고 한다. **markov-매트릭스의 고유값들중 최소한 하나는 1이고 나머지 고유값은 모두 1보다 작다.** 이 사실도 그냥 증명없이 외우자. 

- **임의의 $2\times 2$-행렬 ${\bf A}=rbind(c(a,b),c(c,d))$에서 고정된 고유값 $\lambda^* $에 대한 고유벡터는 $c(-b,a-\lambda^* )$이다.** 이건 그냥 혼자 유추한것인데 상당히 유용하다. 증명은 그렇게 어렵지 않다. 우선 임의의 $2 \times 2$-행렬의 고유값과 고유벡터는 항상 존재한다. 고정된 고유값 $\lambda^* $에 대한 *characteristic polynomial*은 아래와 같이 쓸 수 있다. 
\begin{align}
det\left({\bf A}-\lambda^* {\bf I}\right)=0
\end{align}
앞에서 언급하였듯이 고유치 $\lambda^* $의 정의에 의해서 위의 식은 항상 성립한다. 따라서 행렬 ${\bf A}-\lambda^* {\bf I}$는 *sing*-매트릭스이다. 따라서 0을 고유값으로 가지며 0에 해당하는 고유벡터는 ${\bf v}=(-b,a-\lambda^* )$이다. 그런데 ${\bf v}$는 행렬 ${\bf A}-\lambda^* {\bf I}$에서 고유값 0에 대한 고유벡터이기도 하지만 행렬 ${\bf A}$에서 고유값 $\lambda^* $에 대한 고유벡터이기도 하다. 왜냐하면 
\begin{align}
\left({\bf A}-\lambda^* {\bf I}\right){\bf v}=0
\end{align}
이므로, 
\begin{align}
{\bf Av}=\lambda^* {\bf v}
\end{align}
이기 때문이다. 

- 매트릭스 ${\bf A}=rbind(c(0.5,0.5),c(0.5,0.5))$를 생각하여 보자. $\bf A$는 *sing*-매트릭스이므로 0을 고유값으로 가진다. 또한 $\bf A$는 *markov*-매트릭스 이므로 1을 고유값으로 가진다(그리고 다른 고유값은 모두 1보다 작음). 종합하면 $\bf A$의 고유값은 0과 1이다. 따라서 고유벡터는 $c(-0.5,0.5)$, $c(-0.5,-0.5)$이다. 

- ***symm*-메트릭스의 고유벡터는 서로 직교한다.** 위의 예제를 다시 살펴보자. ${\bf A}=rbind(c(0.5,0.5),c(0.5,0.5))$의 고유벡터는 $c(1,1), c(1,-1)$인데 이 둘은 직교한다. 이것은 그냥 증명없이 외우자. 

- **모든 고유값들의 합은 원래행렬의 trace와 같고 모든 고유값들의 곱은 원래 행렬의 determinent와 같다.** 이 2개의 사실을 연립하여 풀면 거의 모든 $2 \times 2$-매트릭스의 고유값을 쉽게 구할 수 있다. 가령 예를 들면 ${\bf A}=rbind(c(1,2),c(2,4))$인 행렬을 생각하자. $\lambda_1+\lambda_2=5$ 이고 $\lambda_1 \lambda_2=0$ 이다. 따라서 고유값은 0과 5임을 쉽게 유추할 수 있다. 

- 고유값이 반드시 실수일 이유는 없다. ${\bf A_}=rbind(c(0,-1),c(1,0))$을 생각하여 보자. $\lambda_1+\lambda_2=0$ 이고 $\lambda_1 \lambda_2=1$이다. $i$와 $-i$가 해당조건을 만족하므로 ${\bf A}$의 고유값은 $i$와 $-i$이다. 

- ${\bf A_}=rbind(c(0,-1),c(1,0))$의 고유값이 $i$와 $-i$인 것은 약간의 *intuition*을 활용하면 쉽게 유추할 수 있다. $\bf A$는 어떠한 벡터든지 90도 만큼 회전시키는 변환이다. 따라서 ${\bf A}^2$는 어떠한 벡터든지 180도 회전시키는 변환이 된다. 이말은 모든 벡터 $c(a,b)$를 $c(-a,-b)$로 만든다는 의미이다. 즉 ${\bf A}^2=-{\bf I}$이다. 따라서 ${\bf A}^2$의 고유값은 -1이 되고 고유벡터는 ***all non-zero vectors***가 된다. 그리고 $-{\bf A}^2$는 *pd*-매트릭스가 된다. 따라서 아래와 같이 대각화 가능하다. 심지어 $-{\bf A}^2$는 *symm*-매트릭스이므로 ${\bf V}$는 직교행렬이다. 즉 
\begin{align}
\bf -A^2=VDV' ,~~ D=diag(1,1)
\end{align}
이다. 따라서 $\sqrt{D}=diag(-i,i)$
\begin{align}
\bf -A=V\sqrt{D}VV'\sqrt{D}V' 
\end{align}
이므로 
