---
layout: post
title: 야매 강화학습
---

### 그리드세계 

- $4\times 4$개의 격자가 있는 세계를 상상하자. 

- 로봇이 움직일수 있는 16개의 상태를 아래와 같의 정의하자. 
```{r}
s<-1:16
```
편의상 ${\cal S}:=\\{s[1],\dots,s[16]\\}$이라고 정의하자. 

- 액션을 정의하자. 로봇은 동서남북으로 움직일수 있으므로 4가지 행동을 취할 수 있다.
```{r}
a<-c("e","w","s","n")
```
편의상 ${\cal A}:=\\{a[1],\dots,a[4]\\}$라고 정의하자. 

- 특정 상태 $s\in {\cal S}$에서 다른 특정 상태 $s'\in {\cal S}$로 이동할 확률을 모두 정의한 매트릭스 $P$를 만들자. 이것을 transition matrix라고 하자. 이때 $P:{\cal S}\rightarrow {\cal S}$이다. 가령 예를 들면 각 상태로 이동할 확률이 모두 동일하다면 $P$를 아래와 같이 설정한다. 
```{r}
P<-rep(1/16,16*16)
dim(P)<-c(16,16)
```

- 그런데 MDP환경에서는 $P:{\cal A}\times{\cal S} \rightarrow {\cal S}$이므로 아래와 같이 $16 \times  4 \times 16$의 매트릭스로 설정하는 것이 바람직하다. 
```{r}
P<-rep(0,16*4*16)
dim(P)<-c(16,4,16)
``` 
보통 특정환경 $s \in {\cal S}$에서 특정행동 $a \in {\cal A}$을 할때 이동되는 다른 환경 $s' \in {\cal S}$로 갈 확률을 명확하게 알고 있으므로 임의의 $P[s,a,s']$의 값은 아래와 같이 세세하게 다 정의할 수 있다. 
```{r}
P[1,1,1]<- somevalue
...
P[16,4,16]<- somevalue
```

- 보상함수 $r$ 대하여 정의하자.  
  1. 상태 $s[1]$혹은 상태 $s[16]$에 도달하면 5점을 받는다. 
  2. 그리드 밖으로 나가면 -1점을 얻는다. 
  3. 그외에 경우에는 0점을 받는다.  

- 보는것처럼 보상은 상태와 액션에 따른 함수이다. 따라서 $r:{\cal A}\times {\cal S} \rightarrow \mathbb{R}$이다. 만약에 $(s,a) \in {\cal S}\times{\cal A}$에서 다음상태 $s' \in {\cal S}$로 가는 과정이 스토캐스틱하지 않고 결정적이라면, 현재상태 $r(s,a)$는 랜덤변수가 아니다. 따라서 아래와 같이 차원을 정의하고 모든 값을 넣을 수있다.  
```{r}
r<-rep(0,16*4)
dim(r)<-c(16,4)
r[1,1]<- somevalue
...
r[16,4]<- somevalue
```

- 그런데 우리는 MDP환경을 가정하므로, $(s,a) \in {\cal S}\times{\cal A}$에서 다음상태 $s' \in {\cal S}$로 가는 과정이 스토캐스틱하다고 가정할 것이다. 따라서 이 경우에 $r(s,a)$는 확률변수가 된다. 이때 $r$은 $P : {\cal S} \times {\cal A} \rightarrow {\cal S} \rightarrow \mathbb{R}$과 같은 형태가 된다. 

- 특정 상태 $s \in {\cal S}$에서 특정 액션 $a \in {\cal A}$을 선택할 확률을 정의한 매트릭스 $\pi$를 만들자. 이를 정책이라고 하자. 예를들어 각 상태 16개에서 선택할 수 있는 액션의 확률이 모두 동일한 정책 $\pi_1(s,a)$은 아래와 같이 정의할 수 있다. 
```{r}
pi_1<-rep(1/4,16*4)
dim(pi_1)<-c(16,4) 
```

- 특정 상태 $s \in {\cal S}$ 대하여 정책 $\pi_1$가 좋은 정책인지 나쁜 정책인지 평가할 수 있다. 이는 특정상태에서 정책 $\pi_1$을 썼을때 얻게되는 보상의 기대값으로 정의할 수 있다. 이를 정책 $\pi_1$에 대한 가치함수라고 하고 기호로는 $v_{\pi_1}:{\cal S}\rightarrow \mathbb{R}$로 쓴다. 
\begin{align}
v(\pi_1,s):=v_{\pi_1}(s)=\sum_{a\in {\cal A}}\pi_1(s,a)\left(r(s,a)+\gamma \sum_{(s',r) \in {\cal S}\times{\cal R}}P(s,a,s')v(\pi_1,s')\right)
\end{align}

