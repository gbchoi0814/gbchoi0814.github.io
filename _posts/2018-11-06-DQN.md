---
layout: post
title: 야매 강화학습
---

### 그리드세계 

- $4\times 4$개의 격자가 있는 세계를 상상하자. 

- 로봇이 움직일수 있는 16개의 상태를 아래와 같의 정의하자. 
```{r}
s<-1:16
```
편의상 ${\cal S}:=\\{s[1],\dots,s[16]\\}$이라고 정의하자. 

- 액션을 정의하자. 로봇은 동서남북으로 움직일수 있으므로 4가지 행동을 취할 수 있다.
```{r}
a<-c("e","w","s","n")
```
편의상 ${\cal A}:=\\{a[1],\dots,a[4]\\}$라고 정의하자. 

- 특정 상태 $s\in {\cal S}$에서 다른 특정 상태 $s'\in {\cal S}$로 이동할 확률을 모두 정의한 매트릭스 $P$를 만들자. 이것을 transition matrix라고 하자. 이때 $P:{\cal S}\rightarrow {\cal S}$이다. 가령 예를 들면 각 상태로 이동할 확률이 모두 동일하다면 $P$를 아래와 같이 설정한다. 
```{r}
P<-rep(1/16,16*16)
dim(P)<-c(16,16)
```

- 보상함수 $r$ 대하여 정의하자.  
  1. 상태 $s[1]$혹은 상태 $s[16]$에 도달하면 4점 혹은 5점의 점수를 랜덤으로 받는다. 
  2. 그리드 밖으로 나가면 -2점 혹은 -1점의 점수를 랜덤으로 얻는다. 
  3. 바람이 불어서 원하는 상태로 도착하지 못하였을 경우는 -3점의 점수를 얻는다.  
  4. 그외에 경우에는 0점의 점수를 받는다.  

- 보상을 얼마 얻을지 생각하기 위해서는 1)현재상태 2)현재상태에서액션 3)다음상태에 대한 정보가 모두 필요하다. 그리고 1) 2) 3)을 알아도 $r$이 얼마인지 디터미니스틱하게 알 수 있는 것이 아니고 스토캐스틱하게 알 수 있다. 따라서 $r$은 $(s,a,s')$의 값이 모두 given되었을 때 어떠한 분포를 가지는 확률변수이다. 우선 보상이 가지는 값은 6가지 경우이다. 
```{r}
r<-c(-3,-2,-1,0,4,5)
```
편의상 ${\cal R}=\\{r[1],\dots,r[6] \\}$이라고 하자.  

- 강화학습은 보통 MDP환경에서 정의한다. 여기에서 MDP환경은 아래와 같은 특징을 가진다. 
  1. 현재상태 $s \in {\cal S}$에서 어떠한 액션 $a \in {\cal A}$을 취하면 다음상태 $s' \in {\cal S}$ 스토캐스틱하게 넘어간다. 
  2. $(s,a,s')$이 모두 주어져도 $r$이 디터미니스틱하게 결정되는 것은 아니다. 
  
- 이러한 환경은 트랜지션 매트릭스 $P:(s,a) \rightarrow (s',r)$로 정의할 수 있다. 따라서 아래와 같이 트랜지션 매트릭스를 정의한다. 
```{r}
P<-rep(0,4*16*16*6)
dim(P)<-c(4,16,16,6)
P[1,1,1,1]<- somevalue
...
P[16,4,16,6]<- somevalue
```
여기에서 $P[4,16,16,6]$은 상태 $s[16]$에서 적당한 액션 $a[4]$를 취했을때 상태 $s[16]$으로 전환되고 보상 $r[6]$을 받을 확률을 의미한다. 그나마 게임은 룰이 명확하기 때문에 $P$를 정의할 수 있다. 

- 특정 상태 $s \in {\cal S}$에서 특정 액션 $a \in {\cal A}$을 선택할 확률을 정의한 매트릭스 $\pi$를 만들자. 이를 정책이라고 하자. 예를들어 각 상태 16개에서 선택할 수 있는 액션의 확률이 모두 동일한 정책 $\pi_1(s,a)$은 아래와 같이 정의할 수 있다. 
```{r}
pi_1<-rep(1/4,16*4)
dim(pi_1)<-c(16,4) 
```

- 특정 상태 $s \in {\cal S}$ 대하여 정책 $\pi_1$가 좋은 정책인지 나쁜 정책인지 평가할 수 있다. 이는 특정상태에서 정책 $\pi_1$을 썼을때 얻게되는 보상의 기대값으로 정의할 수 있다. 이를 정책 $\pi_1$에 대한 가치함수라고 하고 기호로는 $v_{\pi_1}:{\cal S}\rightarrow \mathbb{R}$로 쓴다. 
\begin{align}
v(\pi_1,s):=v_{\pi_1}(s)=\sum_{a\in {\cal A}}\pi_1(s,a)\left(r(s,a)+\gamma \sum_{(s',r) \in {\cal S}\times{\cal R}}P(s,a,s',)v(\pi_1,s')\right)
\end{align}

