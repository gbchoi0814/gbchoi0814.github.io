---
layout: post
title: 야매 강화학습
---

### 그리드세계 

- $4\times 4$개의 격자가 있는 세계를 상상하자. 

- 로봇이 움직일수 있는 16개의 상태를 아래와 같의 정의하자. 
```{r}
s<-1:16
```
편의상 ${\cal S}:=\\{s[1],\dots,s[16]\\}$이라고 정의하자. 


- 액션을 정의하자. 로봇은 동서남북으로 움직일수 있으므로 4가지 행동을 취할 수 있다.
```{r}
a<-c("e","w","s","n")
```
편의상 ${\cal A}:=\\{a[1],\dots,a[4]\\}$라고 정의하자. 

- 보상함수 $r$ 대하여 정의하자. 
```
  - 상태 s[1]혹은 상태 s[16]에 도달하면 5점을 받는다. 
  - 그리드 밖으로 나가면 -1점을 얻는다. 
  - 그외에 경우에는 0점을 받는다.  
```
보는것처럼 보상은 상태와 액션에 따른 함수이다. 따라서 $r:{\cal A}\times {\cal S} \rightarrow \mathbb{R}$이다. 따라서 아래와 같이 차원을 정하는 것이 바람직하다. 
```{r}
r<-rep(0,16*4)
dim(r)<-c(16,4)
```
각각의 상태와 액션만 주어지면 모든 보상을 정의할 수 있으므로 임의의 $s=1,\dots,16$, $a=1,2,3,4$에 대하여 r[s,a]의 값을 정의할 수 있다.  
```{r}
r[1,1]<- somevalue
...
r[16,4]<- somevalue
```

- 특정 상태 $s\in {\cal S}$에서 다른 특정 상태 $s'\in {\cal S}$로 이동할 확률을 모두 정의한 매트릭스 P를 만들자. 이것을 transition matrix라고 하자. 이때 $P:{\cal S}\rightarrow {\cal S}$이다. 가령 예를 들면 각 상태로 이동할 확률이 모두 동일하다면 P를 아래와 같이 설정한다. 
```{r}
P<-rep(1/16,16*16)
dim(P)<-c(16,16)
```
그런데 MDP환경에서는 $P:{\cal A}\times{\cal S} \rightarrow {\cal S}$이므로 아래와 같이 $16 \times  4 \times 16$의 매트릭스로 설정하는 것이 바람직하다. 
```{r}
P<-rep(0,16*4*16)
dim(P)<-c(16,4,16)
``` 
보통 특정환경 $s \in {\cal S}$에서 특정행동 $a \in {\cal A}$을 할때 이동되는 다른 환경 $s' \in {\cal S}$로 갈 확률을 명확하게 알고 있으므로 임의의 P[s,a,s']의 값은 아래와 같이 세세하게 다 정의할 수 있다. 
```{r}
P[1,1,1]<- somevalue
...
P[16,4,16]<- somevalue
```

- 특정 상태 $s \in {\cal S}$에서 특정 액션 $a \in {\cal A}$을 선택할 확률을 정의한 매트릭스 $\pi$를 만들자. 이를 정책이라고 하자. 예를들어 각 상태 16개에서 선택할 수 있는 액션의 확률이 모두 동일한 정책 $\pi_1(s,a)$은 아래와 같이 정의할 수 있다. 
```{r}
pi_1<-rep(1/4,16*4)
dim(pi_1)<-c(16,4) 
```

- 특정 상태 $s \in {\cal S}$ 대하여 정책 $\pi_1$가 좋은 정책인지 나쁜 정책인지 평가할 수 있다. 이는 특정상태에서 정책 $\pi_1$을 썼을때 얻게되는 보상의 기대값으로 정의할 수 있다. 이를 정책 $\pi_1$에 대한 가치함수라고 하고 기호로는 $v_{\pi}:{\cal S}\rightarrow \mathbb{R}$로 쓴다. 
\begin{align}
v(\pi_1,s):=v_{\pi_1}(s)=\sum_{a\in {\cal A}}pi_1(s,a)r(s,a)+
\end{align}
