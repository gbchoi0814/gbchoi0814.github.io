---
layout: post
title: 야매 강화학습
---

### 그리드세계 

- $4\times 4$개의 격자가 있는 세계를 상상하자. 

- 로봇이 움직일수 있는 16개의 상태를 아래와 같의 정의하자. 
```{r}
s<-1:16
```
편의상 ${\cal S}:=\\{s[1],\dots,s[16]\\}$이라고 정의하자. 

- 액션을 정의하자. 로봇은 동서남북으로 움직일수 있으므로 4가지 행동을 취할 수 있다.
```{r}
a<-c("e","w","s","n")
```
편의상 ${\cal A}:=\\{a[1],\dots,a[4]\\}$라고 정의하자. 

- 특정 상태 $s\in {\cal S}$에서 다른 특정 상태 $s'\in {\cal S}$로 이동할 확률을 모두 정의한 매트릭스 $P$를 만들자. 이것을 transition matrix라고 하자. 이때 $P:{\cal S}\rightarrow {\cal S}$이다. 가령 예를 들면 각 상태로 이동할 확률이 모두 동일하다면 $P$를 아래와 같이 설정한다. 
```{r}
P<-rep(1/16,16*16)
dim(P)<-c(16,16)
```
이경우 $sum(P[1,])=\dots=sum(P[16,])=1$이고, $sum(P[,1])=\dots=sum(P[,16])=1$이다. 즉 우리가 여기에서 정의한 $P[s,s']$는 $P(s'|s)$이다. 

- 강화학습은 보통 MDP에서 정의되므로 트랜지션 매트릭스 $P$는 액션을 고려하여 다음과 같은 함수가 되어야 함이 마땅하다.
\begin{align}
P:{\cal S}\times{\cal A} \rightarrow {\cal S}
\end{align}
 즉 이전상태와 이전상태에서의 액션이 기븐되어야 다음상태를 알 수 있다. 따라서 아래와 같이 트랜지션 매트릭스를 정의한다. 
```{r}
P<-rep(0,4*16*16)
dim(P)<-c(4,16,16)
P[1,1,1]<- somevalue
...
P[16,4,16]<- somevalue
```
여기에서 $P[4,16,16]$은 상태 $s[16]$에서 적당한 액션 $a[4]$를 취했을때 상태 $s[16]$으로 전환될 확률을 의미한다. 그나마 게임은 룰이 명확하기 때문에 $P$를 정의할 수 있다. 새롭게 정의된 $P$는 아래식이 성립한다. 즉 우리가 여기서 정의한 $P[s,a,s']$는 $P(s'|s,a)$이다. 
  - $sum(P[1,1,])=sum(P[1,2,])=sum(P[1,3,])=sum(P[1,4,])=1$
  - $sum(P[16,1,])=sum(P[16,2,])=sum(P[16,3,])=sum(P[16,4,])=1$

- 보상함수 $r$ 대하여 정의하자.  
  1. 상태 $s[1]$혹은 상태 $s[16]$에 도달하면 5점의 점수를 받는다. 
  2. 그리드 밖으로 나가면 -1점의 점수를 얻는다.  
  3. 그외에 경우에는 0점의 점수를 받는다.  
  
- 특정 상태 $s \in {\cal S}$가 기븐되었을 경우 특정 액션 $a \in {\cal A}$을 선택할 확률을 정의한 매트릭스 $\pi$를 만들자. 이를 정책이라고 하자. 예를들어 각 상태 16개에서 선택할 수 있는 액션의 확률이 모두 동일한 정책 $\pi_1(a|s)$은 아래와 같이 정의할 수 있다. 따라서 $\pi_1(a|s)=P(a|s)$이다. 
```{r}
pi_1<-rep(1/4,16*4)
dim(pi_1)<-c(16,4) 
```

- 특정 상태 $s \in {\cal S}$ 대하여 정책 $\pi_1$가 좋은 정책인지 나쁜 정책인지 평가할 수 있다. 이는 특정상태에서 정책 $\pi_1$을 썼을때 얻게되는 보상의 기대값으로 정의할 수 있다. 이를 정책 $\pi_1$에 대한 가치함수라고 하고 기호로는 $v_{\pi_1}:{\cal S}\rightarrow \mathbb{R}$로 쓴다. 
\begin{align}
v(\pi_1,s):=v_{\pi_1}(s)=\sum_{a\in {\cal A}}\pi_1(a|s)\left(r(s,a)+\gamma P(s'|s,a)v_{\pi_1}(s')\right)
\end{align}

