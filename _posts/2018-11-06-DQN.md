---
layout: post
title: 야매 강화학습
---

### 그리드세계 

- $4\times 4$개의 격자가 있는 세계를 상상하자. 

- 로봇이 움직일수 있는 16개의 상태를 아래와 같의 정의하자. 
```{r}
s<-1:16
```

- 보상에 대하여 정의하자. 
  - 상태 s[1]혹은 상태 s[16]에 도달하면 5점을 받는다. 
  - 그리드 밖으로 나가면 -1점을 얻는다. 
  - 그외에 경우에는 0점을 받는다. 

- 액션을 정의하자. 로봇은 동서남북으로 움직일수 있으므로 4가지 행동을 취할 수 있다.
```{r}
a<-c("e","w","s","n")
```

- 특정 상태에서 다른 특정 상태로 이동할 확률을 모두 정의한 매트릭스 P를 만들자. 이것을 transition matrix라고 하자. 가령 예를 들면 각 상태로 이동할 확률이 모두 동일하다면 P를 아래와 같이 설정한다. 
```{r}
P<-rep(1/16,16*16); dim(P)<-c(16,16);
```

- 특정 상태에서 특정 액션을 선택할 확률을 정의한 매트릭스 $\pi$를 만들자. 이를 정책이라고 하자. 예를들어 각 상태 16개에서 선택할 수 있는 액션의 확률이 모두 동일한 정책 $\pi_1(s,a)$은 아래와 같이 정의할 수 있다. 
```{r}
pi_1<-rep(1/4,16*4); dim(pi_1)<-c(16,4) 
```

- 상태 $(s[1],\dots,s[16])$ 어떠한 정책 $\pi_1$이 좋은 정책인지 나쁜 정책인지 평가할 수 있다. 이는 특정상태에서 정책 $\pi_1$을 썼을때 얻게되는 보상의 기대값으로 정의할 수 있다. 이를 정책 $\pi_1$에 대한 가치함수라고 하고 기호로는 $v(\pi_1,s)$로 쓴다. 
\begin{aling}
v(\pi_1,s):=v_{\pi_1}(s)=\sum_{a}pi(s,a)
\end{align}
