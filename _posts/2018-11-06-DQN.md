---
layout: post
title: 야매 강화학습
---

### 그리드세계 

- $4\times 4$개의 격자가 있는 세계를 상상하자. 

- 로봇이 움직일수 있는 16개의 상태를 아래와 같의 정의하자. 
```{r}
s<-1:16
```
편의상 ${\cal S}:=\\{s[1],\dots,s[16]\\}$이라고 정의하자. 

- 액션을 정의하자. 로봇은 동서남북으로 움직일수 있으므로 4가지 행동을 취할 수 있다.
```{r}
a<-c("e","w","s","n")
```
편의상 ${\cal A}:=\\{a[1],\dots,a[4]\\}$라고 정의하자. 

- 특정 상태 $s\in {\cal S}$에서 다른 특정 상태 $s'\in {\cal S}$로 이동할 확률을 모두 정의한 매트릭스 $P$를 만들자. 이것을 transition matrix라고 하자. 이때 $P:{\cal S}\rightarrow {\cal S}$이다. 가령 예를 들면 각 상태로 이동할 확률이 모두 동일하다면 $P$를 아래와 같이 설정한다. 
```{r}
P<-rep(1/16,16*16)
dim(P)<-c(16,16)
```
이경우 $sum(P[1,])=\dots=sum(P[16,])=1$이고, $sum(P[,1])=\dots=sum(P[,16])=1$이다. 즉 우리가 여기에서 정의한 $P[s,s']$는 $P(s'|s)$이다. 

- 강화학습은 보통 MDP에서 정의되므로 트랜지션 매트릭스 $P$는 액션을 고려하여 다음과 같은 함수가 되어야 함이 마땅하다.
\begin{align}
P:{\cal S}\times{\cal A} \rightarrow {\cal S}
\end{align}
 즉 이전상태와 이전상태에서의 액션이 기븐되어야 다음상태를 알 수 있다. 따라서 아래와 같이 트랜지션 매트릭스를 정의한다. 
```{r}
P<-rep(0,4*16*16)
dim(P)<-c(4,16,16)
P[1,1,1]<- somevalue
...
P[16,4,16]<- somevalue
``` 
여기에서 $P[4,16,16]$은 상태 $s[16]$에서 적당한 액션 $a[4]$를 취했을때 상태 $s[16]$으로 전환될 확률을 의미한다. 그나마 게임은 룰이 명확하기 때문에 $P$를 정의할 수 있다. 새롭게 정의된 $P$는 아래식이 성립한다. 즉 우리가 여기서 정의한 $P[s,a,s']$는 $P(s'|s,a)$이다. 
  - $sum(P[1,1,])=sum(P[1,2,])=sum(P[1,3,])=sum(P[1,4,])=1$
  - $sum(P[16,1,])=sum(P[16,2,])=sum(P[16,3,])=sum(P[16,4,])=1$



- 보상함수 $r$ 대하여 정의하자.  
  1. 상태 $s[1]$혹은 상태 $s[16]$에 도달하면 5점의 점수를 받는다. 
  2. 그리드 밖으로 나가면 -1점의 점수를 얻는다. (그리고 다음상태는 이전상태와 동일하다)   
  3. 그외에 경우에는 0점의 점수를 받는다.  
  


- 보상은 16개의 상태에서 4개의 행동을 하는 경우에서 모두 정의할 수 있다. 따라서 보상 $r(s,a)$은 아래와 같이 쓸 수 있다. 
```{r}
r<-rep(0,16*4)
dim(r)<-c(16,4)
```
이때 보상 $r$이 현재상태와 현재상태의액션에 대한 함수이지 다음상태에 대한 함수는 아님을 기억하자. (즉 나중상태로 보상을 완벽히 정의할 수 없다는 의미이다. 가령 예를들어서 로봇이 $s'=(3,4)$의 위치에 있다고 하자. 그런데 $s=(2,4)$이고 $a=a[1]$이어서 $s'=(3,4)$가 된 경우에는 보상이 0이지만 $s=(3,4)$인데 $a=a[4]$와 같이 되어서 그리드밖으로 튀어나간 경우는 보상이 -1이다. 다시말하면 다음상태 $s'=(3,4)$가 같다고 해서 항상 그 보상이 같은 것은 아니다.) 

- 특정 상태 $s \in {\cal S}$가 기븐되었을 경우 특정 액션 $a \in {\cal A}$을 선택할 확률을 정의한 매트릭스 $\pi$를 만들자. 이를 정책이라고 하자. 예를들어 각 상태 16개에서 선택할 수 있는 액션의 확률이 모두 동일한 정책 $\pi_1(a|s)$은 아래와 같이 정의할 수 있다. 
```{r}
pi_1<-rep(1/4,16*4)
dim(pi_1)<-c(16,4) 
```
이때 $\pi_1[s,a]=\pi_1(a|s)=P(a|s)$이다. 

- 특정 상태 $s \in {\cal S}$에 대하여 정책 $\pi_1(a|s)$가 좋은 정책인지 나쁜 정책인지 평가할 수 있다. 이는 특정상태에서 정책 $\pi_1(a|s)$을 썼을때 얻게되는 보상의 기대값으로 정의할 수 있다. 이런식으로 모든 상태 $s \in {\cal S}$에 대하여 정책 $\pi_1(a|s)$를 썼을때 보상의 기대값을 계산할 수 있는데 이를 정책 $\pi_1(a|s)$대한 가치함수라고 하고 기호로는 $v_{\pi_1}:{\cal S}\rightarrow \mathbb{R}$로 쓴다. 
\begin{align}
v(\pi_1,s):=v_{\pi_1}(s)=\sum_{a\in {\cal A}}\pi_1(a|s)\left(r(s,a)+\gamma \sum_{s' \in {\cal S}}P(s'|s,a)v_{\pi_1}(s')\right)
\end{align}

- 특정정책 $\pi_1(a|s)$이 고정되면 가치함수를 계산할 수 있다. 최적가치함수는 최적의정책 $\hat{\pi}(a|s)$로 부터 계산되는 가치함수 $v_{\pi}(s)$이다. 즉 최적의정책을 알고 있다면 최적의가치함수를 계산할 수 있다. 반대로 최적의가치함수값을 알고있으면 그것을 통하여 최적의정책을 찾을 수 있다. 따라서 결국 강화학습의 문제는 결국 ***최적의가치함수를 찾아보자!*** 로 요약된다. 

- 최적의가치함수를 찾는 과정으로 **정책반복알고리즘**이 있다. 이것은 초기에 임의의 정책을 초기화하고 그담에 그것을 바탕으로 가치함수를 계산하고 계산된 가치함수를 활용하여 정책을 업그레이드하고 다시 그것으로 가치함수를 계산하는 식으로 진행한다. 반복이 진행될수록 정책은 점점 최적정책 $\hat{\pi}(a|s)$로 수렴하고 가치함수는 점점 최적가치함수 $v_{\hat{\pi}}(s)$로 수렴할 것이라 기대된다. 

- **정책반복알고리즘**은 매우 시간이 많이 걸린다. 사실 특정한 정책이 결정되었을 경우 가치함수를 계산하는 것이 말처럼 쉬운일이 아니다. 왜냐하면 특정 정책이 결정되었을 경우 가치함수를 계산하려면 순환식을 풀어야 하기 때문이다. 

- 또 다른 방법은 **가치반복알고리즘**이다. 이것은 초기에 임의의 가치함수를 초기화하고 그담에 그것을 바탕으로 
