---
layout: post
title: 야매 강화학습
---

### 그리드세계 

- $4\times 4$개의 격자가 있는 세계를 상상하자. 

- 로봇이 움직일수 있는 16개의 상태를 아래와 같의 정의하자. 
```{r}
s<-1:16
```
편의상 ${\cal S}:=\\{s[1],\dots,s[16]\\}$이라고 정의하자. 

- 액션을 정의하자. 로봇은 동서남북으로 움직일수 있으므로 4가지 행동을 취할 수 있다.
```{r}
a<-c("e","w","s","n")
```
편의상 ${\cal A}:=\\{a[1],\dots,a[4]\\}$라고 정의하자. 

- 특정 상태 $s\in {\cal S}$에서 다른 특정 상태 $s'\in {\cal S}$로 이동할 확률을 모두 정의한 매트릭스 $P$를 만들자. 이것을 transition matrix라고 하자. 이때 $P:{\cal S}\rightarrow {\cal S}$이다. 가령 예를 들면 각 상태로 이동할 확률이 모두 동일하다면 $P$를 아래와 같이 설정한다. 
```{r}
P<-rep(1/16,16*16)
dim(P)<-c(16,16)
```
이경우 $sum(P[1,])=\dots=sum(P[16,])=1$이고, $sum(P[,1])=\dots=sum(P[,16])=1$이다. 즉 우리가 여기에서 정의한 $P[s,s']$는 $P(s'|s)$이다. 

- 강화학습은 보통 MDP에서 정의되므로 트랜지션 매트릭스 $P$는 액션을 고려하여 다음과 같은 함수가 되어야 함이 마땅하다.
\begin{align}
P:{\cal S}\times{\cal A} \rightarrow {\cal S}
\end{align}
 즉 이전상태와 이전상태에서의 액션이 기븐되어야 다음상태를 알 수 있다. 따라서 아래와 같이 트랜지션 매트릭스를 정의한다. 
```{r}
P<-rep(0,4*16*16)
dim(P)<-c(4,16,16)
P[1,1,1]<- somevalue
...
P[16,4,16]<- somevalue
``` 
여기에서 $P[4,16,16]$은 상태 $s[16]$에서 적당한 액션 $a[4]$를 취했을때 상태 $s[16]$으로 전환될 확률을 의미한다. 그나마 게임은 룰이 명확하기 때문에 $P$를 정의할 수 있다. 새롭게 정의된 $P$는 아래식이 성립한다. 즉 우리가 여기서 정의한 $P[s,a,s']$는 $P(s'|s,a)$이다. 
  - $sum(P[1,1,])=sum(P[1,2,])=sum(P[1,3,])=sum(P[1,4,])=1$
  - $sum(P[16,1,])=sum(P[16,2,])=sum(P[16,3,])=sum(P[16,4,])=1$



- 보상함수 $r$ 대하여 정의하자.  
  1. 상태 $s[1]$혹은 상태 $s[16]$에 도달하면 5점의 점수를 받는다. 
  2. 그리드 밖으로 나가면 -1점의 점수를 얻는다. (그리고 다음상태는 이전상태와 동일하다)   
  3. 그외에 경우에는 0점의 점수를 받는다.  
  


- 보상은 16개의 상태에서 4개의 행동을 하는 경우에서 모두 정의할 수 있다. 따라서 보상 $r(s,a)$은 아래와 같이 쓸 수 있다. 
```{r}
r<-rep(0,16*4)
dim(r)<-c(16,4)
```
이때 보상 $r$이 현재상태와 현재상태의액션에 대한 함수이지 다음상태에 대한 함수는 아님을 기억하자. (즉 나중상태로 보상을 완벽히 정의할 수 없다는 의미이다. 가령 예를들어서 로봇이 $s'=(3,4)$의 위치에 있다고 하자. 그런데 $s=(2,4)$이고 $a=a[1]$이어서 $s'=(3,4)$가 된 경우에는 보상이 0이지만 $s=(3,4)$인데 $a=a[4]$와 같이 되어서 그리드밖으로 튀어나간 경우는 보상이 -1이다. 다시말하면 다음상태 $s'=(3,4)$가 같다고 해서 항상 그 보상이 같은 것은 아니다.) 

- 특정 상태 $s \in {\cal S}$가 기븐되었을 경우 특정 액션 $a \in {\cal A}$을 선택할 확률을 정의한 매트릭스 $\pi$를 만들자. 이를 정책이라고 하자. 예를들어 각 상태 16개에서 선택할 수 있는 액션의 확률이 모두 동일한 정책 $\pi_1(a|s)$은 아래와 같이 정의할 수 있다. 
```{r}
pi_1<-rep(1/4,16*4)
dim(pi_1)<-c(16,4) 
```
이때 $\pi_1[s,a]=\pi_1(a|s)=P(a|s)$이다. 

- 특정 상태 $s \in {\cal S}$에 대하여 정책 $\pi_1(a|s)$가 좋은 정책인지 나쁜 정책인지 평가할 수 있다. 이는 특정상태에서 정책 $\pi_1(a|s)$을 썼을때 얻게되는 보상의 기대값으로 정의할 수 있다. 이런식으로 모든 상태 $s \in {\cal S}$에 대하여 정책 $\pi_1(a|s)$를 썼을때 보상의 기대값을 계산할 수 있는데 이를 정책 $\pi_1(a|s)$대한 가치함수라고 하고 기호로는 $v_{\pi_1}:{\cal S}\rightarrow \mathbb{R}$로 쓴다. 
\begin{align}
v(\pi_1,s):=v_{\pi_1}(s)=\sum_{a\in {\cal A}}\pi_1(a\vert s)\left(r(s,a)+\gamma \sum_{s' \in {\cal S}}P(s' \vert s,a)v_{\pi_1}(s')\right)
\end{align}

- 특정정책 $\pi_1(a \vert s)$이 고정되면 가치함수를 계산할 수 있다. 최적가치함수는 최적의정책 $\hat{\pi}(a \vert s)$로 부터 계산되는 가치함수 $v_{\pi}(s)$이다. 즉 최적의정책을 알고 있다면 최적의가치함수를 계산할 수 있다. 반대로 최적의가치함수값을 알고있으면 그것을 통하여 최적의정책을 찾을 수 있다. 따라서 결국 강화학습의 문제는 결국 ***최적의가치함수를 찾아보자!*** 로 요약된다. 

- 최적의가치함수를 찾는 과정으로 **정책반복알고리즘**이 있다. 이것은 초기에 임의의 정책을 초기화하고 그담에 그것을 바탕으로 가치함수를 계산하고 계산된 가치함수를 활용하여 정책을 업그레이드하고 다시 그것으로 가치함수를 계산하는 식으로 진행한다. 반복이 진행될수록 정책은 점점 최적정책 $\hat{\pi}(a \vert s)$로 수렴하고 가치함수는 점점 최적가치함수 $v_{\hat{\pi}}(s)$로 수렴할 것이라 기대된다. 

- **정책반복알고리즘**은 매우 시간이 많이 걸린다. 사실 특정한 정책이 결정되었을 경우 가치함수를 계산하는 것이 말처럼 쉬운일이 아니다. 왜냐하면 특정 정책이 결정되었을 경우 가치함수를 계산하려면 순환식을 풀어야 하기 때문이다. 이런식으로 풀면 먼저 목표지점에 인접한 셀들의 가치함수가 먼저 계산되고 이 값들이 바깥의 상태로 전파되면서 값들이 결정된다. 

- 가령 예를 들어서 모든 셀에서 동서남북 랜덤으로 움직이는 정책 쓴다고 하자. 모든셀의 가치는 처음에 $0$으로 초기화 하자. 목표지점은 $(1,1)$과 $(4,4)$ 2군데 이다. 이 정책에 의해서 상태$(1,2)$가 가지는 평균가치는 $\frac{5+0+0-1}{4}=1$으로 계산된다. 요런식으로 모든 셀에 대한 가치를 구할 수 있다. 

- 좀 더 스마트하게 가치를 계산할 수 있다. 현재 상황은 $P(s'|s,a)$를 완벽하게 알고 있는 상황이기 때문에 현재상태 $(1,2)$에서 어떤 액션을 취해야 셀 $(1,2)$의 가치가 최적화 되는지 알 수 있다. (***남쪽으로 가야한다!!***) 어떻게 행동해야 최적인지 이미 계산가능한 상황에서 정책 $\pi_1$를 수동적으로 따르는 것은 어리석으므로 $\pi_1$에서 $(1,2)$에 해당하는 부분만 바꿔서 수정하고 그거에 따른 가치함수를 계산한다. 즉 $\pi_1(a \vert s=(1,2))$를 기존의 동서남북모두에 $1/4$의 확률을 주는것에서 남쪽으로 가는 액션에만 $1$의 확률을 주게 바꾼다. 그리고 바뀐정책을 기반하여 가치함수를 계산한다. (계산값은 $5$이다.) 즉 가치함수를 아래의 식으로 업데이트한다. 
\begin{align}
v(s)^{(t+1)}=\max_{a \in {\cal A}^{(t)}(s)\pi(a \vert s)\left(r(s,a)+\gamma \sum_{s' \in {\cal S}}P(s' \vert s,a)v^{(t)}(s')\right)
\end{align}

- 이때 ${\cal A}^{(t)}(s)$는 현재시점 $t$에서 정책을 고려하였을때 상태 $s$에서 선택가능한 액션들의 집합을 의미한다. 식
\begin{align}
\max_{a \in {\cal A}^{(t)}(s)\pi(a \vert s)\left(r(s,a)+\gamma \sum_{s' \in {\cal S}P(s' \vert s,a)v^{(t)}(s')\right)
\end{align}
을 계산하여 $v(s)^{(t+1)}$를 업데이트 할 때 ${\cal A}^{(t)}(s)$의 값도 ${\cal A}^{(t+1)}(s)$로 업데이트한다. 이러한 방식을 **가치반복알고리즘**이라고 한다. 

- **정책반복알고리즘**은 최적의정책과 최적가치함수를 동시에 개선하는 방식이지만 **가치반복알고리즘**은 최적가치함수를 먼저 찾고 그러부터 최적의정책을 찾는 방식이다. 따라서 **가치반복알고리즘**이 더 빠르다. 

